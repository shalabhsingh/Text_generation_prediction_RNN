{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development and training\n",
    "In this notebook we will develop a Recurrent Neural Network (RNN) using LSTM nodes, so as to predict the next character, given a sequence of 100 characters.\n",
    "\n",
    "First of all, we import the numpy and keras modules, important for storing data and defining the model respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the model now. The model is given an input of 100 character sequences and it outputs the respective probabilities with which a character can succeed the input sequence. The model consists of 3 hidden layers. The first two hidden layers consist of 256 LSTM cells, and the second layer is fully connected to the third layer. The number of neurons in the third layer is same as the number of unique characters in the training set. The neurons in the third layer, use softmax activation so as to convert their outputs into respective probabilities. The loss used is Categorical cross entropy and the optimizer used is Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildmodel(VOCABULARY):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape = (SEQ_LENGTH, 1), return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(VOCABULARY, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the training data. The children book \"Alice's Adventures in Wonderland\" written by Lewis Caroll has been used as the training data in this project. The ebook can be downloaded from [here](http://www.gutenberg.org/ebooks/11?msg=welcome_stranger) in Plain Text UTF-8 format. The downloaded book has been stored in the root directory with the name 'wonderland.txt'. We open this book using the open command and convert all characters into lowercase (so as to reduce the number of characters in the vocabulary, making it easier to learn for the model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('wonderland.txt', encoding = 'utf8')\n",
    "raw_text = file.read()    #you need to read further characters as well\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we store all the distinct characters occurying in the book in the chars variable. We also remove some of the rare characters (stored in bad-chars) from the book. The final vocabulary of the book is printed at the end of code segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”', '\\ufeff']\n",
      "['\\n', ' ', '!', '$', '%', '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "print(chars)\n",
    "\n",
    "bad_chars = ['#', '*', '@', '_', '\\ufeff']\n",
    "for i in range(len(bad_chars)):\n",
    "    raw_text = raw_text.replace(bad_chars[i],\"\")\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we summarize the entire book to find that the book consists of a total of 163,721 characters (which is relatively small) and the final number of characters in the vocabulary is 56."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length = 163721\n",
      "No. of characters = 56\n"
     ]
    }
   ],
   "source": [
    "text_length = len(raw_text)\n",
    "char_length = len(chars)\n",
    "VOCABULARY = char_length\n",
    "print(\"Text length = \" + str(text_length))\n",
    "print(\"No. of characters = \" + str(char_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to create our dataset in the form the model will need. So, we create an input window of 100 characters (SEQ_LENGTH = 100) and shift the window one character at a time until we reach the end of the book. An encoding is used, so as to map each of the characters into it's corresponding location in the vocabulary. Each time the input window contains a new sequence, it is converted into integers, using this encoding and appended to the input list input_strings. For all such input windows, the character just following the sequence is appended to the output list output_strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "input_strings = []\n",
    "output_strings = []\n",
    "\n",
    "for i in range(len(raw_text) - SEQ_LENGTH):\n",
    "    X_text = raw_text[i: i + SEQ_LENGTH]\n",
    "    X = [char_to_int[char] for char in X_text]\n",
    "    input_strings.append(X)    \n",
    "    Y = raw_text[i + SEQ_LENGTH]\n",
    "    output_strings.append(char_to_int[Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the input_strings and output_strings lists are converted into a numpy array of required dimensions, so that they can be fed to the model for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163621, 100, 1)\n",
      "(163621, 56)\n"
     ]
    }
   ],
   "source": [
    "length = len(input_strings)\n",
    "input_strings = np.array(input_strings)\n",
    "input_strings = np.reshape(input_strings, (input_strings.shape[0], input_strings.shape[1], 1))\n",
    "input_strings = input_strings/float(VOCABULARY)\n",
    "\n",
    "output_strings = np.array(output_strings)\n",
    "output_strings = np_utils.to_categorical(output_strings)\n",
    "print(input_strings.shape)\n",
    "print(output_strings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally the model is build and then fitted for training. The model is trained for 50 epochs and a batch size of 128 sequences has been used. After every epoch, the current model state is saved if the model has the least loss encountered till that time. We can see that the training loss decreases in almost every epoch till 50 epochs and probably there is further scope to improve the model.\n",
    "\n",
    "The total training time is ~90 hours on a CPU and ~12 hours if performed on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 2.9082Epoch 00000: loss improved from inf to 2.90829, saving model to saved_models/weights-improvement-00-2.9083.hdf5\n",
      "163621/163621 [==============================] - 6029s - loss: 2.9083  \n",
      "Epoch 2/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 2.6113Epoch 00001: loss improved from 2.90829 to 2.61121, saving model to saved_models/weights-improvement-01-2.6112.hdf5\n",
      "163621/163621 [==============================] - 5779s - loss: 2.6112  \n",
      "Epoch 3/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 2.4221Epoch 00002: loss improved from 2.61121 to 2.42212, saving model to saved_models/weights-improvement-02-2.4221.hdf5\n",
      "163621/163621 [==============================] - 5795s - loss: 2.4221  \n",
      "Epoch 4/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 2.2724Epoch 00003: loss improved from 2.42212 to 2.27221, saving model to saved_models/weights-improvement-03-2.2722.hdf5\n",
      "163621/163621 [==============================] - 5787s - loss: 2.2722  \n",
      "Epoch 5/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 2.1630Epoch 00004: loss improved from 2.27221 to 2.16297, saving model to saved_models/weights-improvement-04-2.1630.hdf5\n",
      "163621/163621 [==============================] - 5905s - loss: 2.1630  \n",
      "Epoch 6/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 2.0790Epoch 00005: loss improved from 2.16297 to 2.07899, saving model to saved_models/weights-improvement-05-2.0790.hdf5\n",
      "163621/163621 [==============================] - 5808s - loss: 2.0790  \n",
      "Epoch 7/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 2.0131Epoch 00006: loss improved from 2.07899 to 2.01314, saving model to saved_models/weights-improvement-06-2.0131.hdf5\n",
      "163621/163621 [==============================] - 5830s - loss: 2.0131  \n",
      "Epoch 8/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.9568Epoch 00007: loss improved from 2.01314 to 1.95677, saving model to saved_models/weights-improvement-07-1.9568.hdf5\n",
      "163621/163621 [==============================] - 5908s - loss: 1.9568  \n",
      "Epoch 9/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.9060Epoch 00008: loss improved from 1.95677 to 1.90604, saving model to saved_models/weights-improvement-08-1.9060.hdf5\n",
      "163621/163621 [==============================] - 5839s - loss: 1.9060  \n",
      "Epoch 10/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.8646Epoch 00009: loss improved from 1.90604 to 1.86468, saving model to saved_models/weights-improvement-09-1.8647.hdf5\n",
      "163621/163621 [==============================] - 5928s - loss: 1.8647  \n",
      "Epoch 11/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.8242Epoch 00010: loss improved from 1.86468 to 1.82409, saving model to saved_models/weights-improvement-10-1.8241.hdf5\n",
      "163621/163621 [==============================] - 6025s - loss: 1.8241  \n",
      "Epoch 12/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.7912Epoch 00011: loss improved from 1.82409 to 1.79117, saving model to saved_models/weights-improvement-11-1.7912.hdf5\n",
      "163621/163621 [==============================] - 7378s - loss: 1.7912  \n",
      "Epoch 13/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.7588Epoch 00012: loss improved from 1.79117 to 1.75886, saving model to saved_models/weights-improvement-12-1.7589.hdf5\n",
      "163621/163621 [==============================] - 6112s - loss: 1.7589  \n",
      "Epoch 14/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.7300Epoch 00013: loss improved from 1.75886 to 1.73003, saving model to saved_models/weights-improvement-13-1.7300.hdf5\n",
      "163621/163621 [==============================] - 6569s - loss: 1.7300  \n",
      "Epoch 15/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.7038Epoch 00014: loss improved from 1.73003 to 1.70380, saving model to saved_models/weights-improvement-14-1.7038.hdf5\n",
      "163621/163621 [==============================] - 6370s - loss: 1.7038  \n",
      "Epoch 16/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.6758Epoch 00015: loss improved from 1.70380 to 1.67590, saving model to saved_models/weights-improvement-15-1.6759.hdf5\n",
      "163621/163621 [==============================] - 5909s - loss: 1.6759  \n",
      "Epoch 17/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.6553Epoch 00016: loss improved from 1.67590 to 1.65520, saving model to saved_models/weights-improvement-16-1.6552.hdf5\n",
      "163621/163621 [==============================] - 5849s - loss: 1.6552  \n",
      "Epoch 18/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.6344Epoch 00017: loss improved from 1.65520 to 1.63442, saving model to saved_models/weights-improvement-17-1.6344.hdf5\n",
      "163621/163621 [==============================] - 5852s - loss: 1.6344  \n",
      "Epoch 19/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.6143Epoch 00018: loss improved from 1.63442 to 1.61431, saving model to saved_models/weights-improvement-18-1.6143.hdf5\n",
      "163621/163621 [==============================] - 5861s - loss: 1.6143  \n",
      "Epoch 20/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.5952Epoch 00019: loss improved from 1.61431 to 1.59517, saving model to saved_models/weights-improvement-19-1.5952.hdf5\n",
      "163621/163621 [==============================] - 5860s - loss: 1.5952  \n",
      "Epoch 21/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.5765Epoch 00020: loss improved from 1.59517 to 1.57645, saving model to saved_models/weights-improvement-20-1.5765.hdf5\n",
      "163621/163621 [==============================] - 6012s - loss: 1.5765  \n",
      "Epoch 22/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.5634Epoch 00021: loss improved from 1.57645 to 1.56339, saving model to saved_models/weights-improvement-21-1.5634.hdf5\n",
      "163621/163621 [==============================] - 5941s - loss: 1.5634  \n",
      "Epoch 23/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.5457Epoch 00022: loss improved from 1.56339 to 1.54577, saving model to saved_models/weights-improvement-22-1.5458.hdf5\n",
      "163621/163621 [==============================] - 6032s - loss: 1.5458  \n",
      "Epoch 24/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.5333Epoch 00023: loss improved from 1.54577 to 1.53337, saving model to saved_models/weights-improvement-23-1.5334.hdf5\n",
      "163621/163621 [==============================] - 6149s - loss: 1.5334  \n",
      "Epoch 25/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.5203Epoch 00024: loss improved from 1.53337 to 1.52028, saving model to saved_models/weights-improvement-24-1.5203.hdf5\n",
      "163621/163621 [==============================] - 6049s - loss: 1.5203  \n",
      "Epoch 26/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.5040Epoch 00025: loss improved from 1.52028 to 1.50402, saving model to saved_models/weights-improvement-25-1.5040.hdf5\n",
      "163621/163621 [==============================] - 5931s - loss: 1.5040  \n",
      "Epoch 27/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4942Epoch 00026: loss improved from 1.50402 to 1.49413, saving model to saved_models/weights-improvement-26-1.4941.hdf5\n",
      "163621/163621 [==============================] - 5933s - loss: 1.4941  \n",
      "Epoch 28/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4827Epoch 00027: loss improved from 1.49413 to 1.48262, saving model to saved_models/weights-improvement-27-1.4826.hdf5\n",
      "163621/163621 [==============================] - 6090s - loss: 1.4826  \n",
      "Epoch 29/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4733Epoch 00028: loss improved from 1.48262 to 1.47327, saving model to saved_models/weights-improvement-28-1.4733.hdf5\n",
      "163621/163621 [==============================] - 6265s - loss: 1.4733  \n",
      "Epoch 30/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4652Epoch 00029: loss improved from 1.47327 to 1.46526, saving model to saved_models/weights-improvement-29-1.4653.hdf5\n",
      "163621/163621 [==============================] - 5951s - loss: 1.4653  \n",
      "Epoch 31/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4517Epoch 00030: loss improved from 1.46526 to 1.45170, saving model to saved_models/weights-improvement-30-1.4517.hdf5\n",
      "163621/163621 [==============================] - 5929s - loss: 1.4517  \n",
      "Epoch 32/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4405Epoch 00031: loss improved from 1.45170 to 1.44058, saving model to saved_models/weights-improvement-31-1.4406.hdf5\n",
      "163621/163621 [==============================] - 5891s - loss: 1.4406  \n",
      "Epoch 33/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4370Epoch 00032: loss improved from 1.44058 to 1.43706, saving model to saved_models/weights-improvement-32-1.4371.hdf5\n",
      "163621/163621 [==============================] - 5889s - loss: 1.4371  \n",
      "Epoch 34/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4256Epoch 00033: loss improved from 1.43706 to 1.42556, saving model to saved_models/weights-improvement-33-1.4256.hdf5\n",
      "163621/163621 [==============================] - 5878s - loss: 1.4256  \n",
      "Epoch 35/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4179Epoch 00034: loss improved from 1.42556 to 1.41779, saving model to saved_models/weights-improvement-34-1.4178.hdf5\n",
      "163621/163621 [==============================] - 5911s - loss: 1.4178  \n",
      "Epoch 36/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4141Epoch 00035: loss improved from 1.41779 to 1.41406, saving model to saved_models/weights-improvement-35-1.4141.hdf5\n",
      "163621/163621 [==============================] - 5878s - loss: 1.4141  \n",
      "Epoch 37/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4024Epoch 00036: loss improved from 1.41406 to 1.40237, saving model to saved_models/weights-improvement-36-1.4024.hdf5\n",
      "163621/163621 [==============================] - 5968s - loss: 1.4024  \n",
      "Epoch 38/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.4012Epoch 00037: loss improved from 1.40237 to 1.40118, saving model to saved_models/weights-improvement-37-1.4012.hdf5\n",
      "163621/163621 [==============================] - 5886s - loss: 1.4012  \n",
      "Epoch 39/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3901Epoch 00038: loss improved from 1.40118 to 1.39005, saving model to saved_models/weights-improvement-38-1.3901.hdf5\n",
      "163621/163621 [==============================] - 5928s - loss: 1.3901  \n",
      "Epoch 40/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3892Epoch 00039: loss improved from 1.39005 to 1.38922, saving model to saved_models/weights-improvement-39-1.3892.hdf5\n",
      "163621/163621 [==============================] - 5916s - loss: 1.3892  \n",
      "Epoch 41/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3841Epoch 00040: loss improved from 1.38922 to 1.38414, saving model to saved_models/weights-improvement-40-1.3841.hdf5\n",
      "163621/163621 [==============================] - 5937s - loss: 1.3841  \n",
      "Epoch 42/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3728Epoch 00041: loss improved from 1.38414 to 1.37282, saving model to saved_models/weights-improvement-41-1.3728.hdf5\n",
      "163621/163621 [==============================] - 5884s - loss: 1.3728  \n",
      "Epoch 43/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3721Epoch 00042: loss improved from 1.37282 to 1.37206, saving model to saved_models/weights-improvement-42-1.3721.hdf5\n",
      "163621/163621 [==============================] - 5901s - loss: 1.3721  \n",
      "Epoch 44/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3688Epoch 00043: loss improved from 1.37206 to 1.36877, saving model to saved_models/weights-improvement-43-1.3688.hdf5\n",
      "163621/163621 [==============================] - 5957s - loss: 1.3688  \n",
      "Epoch 45/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3622Epoch 00044: loss improved from 1.36877 to 1.36229, saving model to saved_models/weights-improvement-44-1.3623.hdf5\n",
      "163621/163621 [==============================] - 5943s - loss: 1.3623  \n",
      "Epoch 46/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3565Epoch 00045: loss improved from 1.36229 to 1.35654, saving model to saved_models/weights-improvement-45-1.3565.hdf5\n",
      "163621/163621 [==============================] - 5912s - loss: 1.3565  \n",
      "Epoch 47/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3596Epoch 00046: loss did not improve\n",
      "163621/163621 [==============================] - 5912s - loss: 1.3596  \n",
      "Epoch 48/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3492Epoch 00047: loss improved from 1.35654 to 1.34926, saving model to saved_models/weights-improvement-47-1.3493.hdf5\n",
      "163621/163621 [==============================] - 5919s - loss: 1.3493  \n",
      "Epoch 49/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3567Epoch 00048: loss did not improve\n",
      "163621/163621 [==============================] - 5955s - loss: 1.3567  \n",
      "Epoch 50/50\n",
      "163584/163621 [============================>.] - ETA: 1s - loss: 1.3420Epoch 00049: loss improved from 1.34926 to 1.34199, saving model to saved_models/weights-improvement-49-1.3420.hdf5\n",
      "163621/163621 [==============================] - 5937s - loss: 1.3420  \n"
     ]
    }
   ],
   "source": [
    "model = buildmodel(VOCABULARY)\n",
    "filepath=\"saved_models/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(input_strings, output_strings, epochs = 50, batch_size = 128, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that our model has been trained, we can use it for different generating texts as well as predicting next word, which is what we will do in the next notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
